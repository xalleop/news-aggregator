#!/usr/bin/env python3
"""
–ù–æ–≤–æ—Å—Ç–Ω–æ–π –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ RSS-—Ñ–∏–¥–æ–≤
–í–µ—Ä—Å–∏—è –±–µ–∑ API - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
"""

import feedparser
import yaml
import json
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict
from collections import defaultdict
import pytz

class NewsAggregator:
    """–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π"""
    
    def __init__(self, feeds_config: str, criteria_config: str):
        self.feeds_config = self.load_config(feeds_config)
        self.criteria = self.load_config(criteria_config)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤
        Path("reports").mkdir(exist_ok=True)
    
    def load_config(self, config_path: str) -> dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"‚úì –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {config_path}")
            return config
        except FileNotFoundError:
            print(f"‚úó –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")
            raise
        except yaml.YAMLError as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ YAML: {e}")
            raise
    
    def get_enabled_feeds(self) -> Dict[str, dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö RSS-—Ñ–∏–¥–æ–≤"""
        feeds = {}
        
        for category in ['general_news', 'tech_news', 'ai_and_regulation']:
            if category in self.feeds_config:
                for name, data in self.feeds_config[category].items():
                    if data.get('enabled', True):
                        feeds[name] = {
                            'url': data['url'],
                            'tags': data.get('tags', []),
                            'category': category
                        }
        
        return feeds
    
    def fetch_rss_feed(self, source_name: str, feed_url: str, hours_back: int) -> List[dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –æ–¥–Ω–æ–≥–æ RSS-—Ñ–∏–¥–∞"""
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        articles = []
        
        try:
            feed = feedparser.parse(feed_url)
            
            for entry in feed.entries:
                # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                try:
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        pub_date = datetime(*entry.updated_parsed[:6])
                    else:
                        pub_date = datetime.now()
                except:
                    pub_date = datetime.now()
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                if pub_date > cutoff_time:
                    articles.append({
                        'title': entry.title,
                        'url': entry.link,
                        'source': source_name,
                        'published': pub_date.isoformat(),
                        'description': entry.get('summary', entry.get('description', ''))
                    })
            
            return articles
            
        except Exception as e:
            print(f"‚úó {source_name:30} ‚Üí –û—à–∏–±–∫–∞: {str(e)[:50]}")
            return []
    
    def fetch_all_news(self, hours_back: int = None) -> List[dict]:
        """–°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ñ–∏–¥–æ–≤"""
        if hours_back is None:
            hours_back = self.feeds_config.get('filters', {}).get('hours_back', 24)
        
        all_articles = []
        feeds = self.get_enabled_feeds()
        
        print(f"\n{'‚îÄ'*70}")
        print(f"–°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {len(feeds)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        print(f"–ü–µ—Ä–∏–æ–¥: –ø–æ—Å–ª–µ–¥–Ω–∏–µ {hours_back} —á–∞—Å–æ–≤")
        print(f"{'‚îÄ'*70}\n")
        
        for source_name, feed_data in feeds.items():
            articles = self.fetch_rss_feed(source_name, feed_data['url'], hours_back)
            
            if articles:
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥–∏ –∫ –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–µ
                for article in articles:
                    article['tags'] = feed_data.get('tags', [])
                    article['category'] = feed_data.get('category', 'unknown')
                
                all_articles.extend(articles)
                print(f"‚úì {source_name:30} ‚Üí {len(articles):3} —Å—Ç–∞—Ç–µ–π")
            else:
                print(f"‚úó {source_name:30} ‚Üí   0 —Å—Ç–∞—Ç–µ–π")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        unique_articles = {a['url']: a for a in all_articles}.values()
        
        print(f"\n{'‚îÄ'*70}")
        print(f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_articles)} —Å—Ç–∞—Ç–µ–π")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(unique_articles)} —Å—Ç–∞—Ç–µ–π")
        print(f"{'‚îÄ'*70}\n")
        
        return list(unique_articles)
    
    def fetch_google_news(self, hours_back: int) -> List[dict]:
        """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google News RSS"""
        google_config = self.feeds_config.get('google_news', {})
        
        if not google_config.get('enabled', False):
            return []
        
        topics = google_config.get('topics', [])
        if not topics:
            return []
        
        google_news_base = 'https://news.google.com/rss/search?q={}&hl=ru&gl=RU&ceid=RU:ru'
        articles = []
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        
        print(f"{'‚îÄ'*70}")
        print(f"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫: Google News")
        print(f"{'‚îÄ'*70}\n")
        
        for topic in topics:
            try:
                url = google_news_base.format(topic)
                feed = feedparser.parse(url)
                count = 0
                
                for entry in feed.entries[:20]:  # –ú–∞–∫—Å–∏–º—É–º 20 –Ω–∞ —Ç–µ–º—É
                    try:
                        pub_date = datetime(*entry.published_parsed[:6])
                    except:
                        pub_date = datetime.now()
                    
                    if pub_date > cutoff_time:
                        articles.append({
                            'title': entry.title,
                            'url': entry.link,
                            'source': 'Google News',
                            'published': pub_date.isoformat(),
                            'description': entry.get('summary', ''),
                            'tags': ['google_news'],
                            'category': 'google_news',
                            'search_topic': topic
                        })
                        count += 1
                
                print(f"‚úì '{topic}' ‚Üí {count} —Å—Ç–∞—Ç–µ–π")
                
            except Exception as e:
                print(f"‚úó '{topic}' ‚Üí –û—à–∏–±–∫–∞: {str(e)[:50]}")
        
        print(f"\nGoogle News: {len(articles)} —Å—Ç–∞—Ç–µ–π\n")
        return articles
    
    def filter_by_keywords(self, articles: List[dict]) -> List[dict]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        filters = self.feeds_config.get('filters', {})
        keywords = filters.get('keywords', [])
        exclude_keywords = filters.get('exclude_keywords', [])
        
        if not keywords:
            print("–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ (–Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤)\n")
            # –î–∞–∂–µ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ matched_keywords
            for article in articles:
                article['matched_keywords'] = []
            return articles
        
        filtered = []
        
        for article in articles:
            text = f"{article['title']} {article['description']}".lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
            if exclude_keywords and any(kw.lower() in text for kw in exclude_keywords):
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∫–ª—é—á–µ–Ω–∏—è
            if any(kw.lower() in text for kw in keywords):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º, –∫–∞–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã
                article['matched_keywords'] = [
                    kw for kw in keywords if kw.lower() in text
                ]
                filtered.append(article)
        
        print(f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {len(filtered)} –∏–∑ {len(articles)} —Å—Ç–∞—Ç–µ–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –∫—Ä–∏—Ç–µ—Ä–∏—è–º\n")
        return filtered
    
    def group_articles(self, articles: List[dict]) -> Dict[str, List[dict]]:
        """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å—Ç–∞—Ç–µ–π –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        groups = {
            'by_source': defaultdict(list),
            'by_category': defaultdict(list),
            'by_keyword': defaultdict(list)
        }
        
        for article in articles:
            groups['by_source'][article['source']].append(article)
            groups['by_category'][article.get('category', 'unknown')].append(article)
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            for keyword in article.get('matched_keywords', []):
                groups['by_keyword'][keyword].append(article)
        
        return groups
    
    def generate_text_report(self, articles: List[dict], groups: dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ–ª–æ–≤–µ–∫–æ-—á–∏—Ç–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        moscow_tz = pytz.timezone('Europe/Moscow')
        timestamp = datetime.now(moscow_tz)
        
        report = f"""
{'='*70}
–ù–û–í–û–°–¢–ù–û–ô –î–ê–ô–î–ñ–ï–°–¢
{'='*70}
–î–∞—Ç–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è: {timestamp.strftime('%d.%m.%Y %H:%M')}
–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}
–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(groups['by_source'])}
{'='*70}

"""
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        report += f"\n{'‚îÄ'*70}\n"
        report += "–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ò–°–¢–û–ß–ù–ò–ö–ê–ú\n"
        report += f"{'‚îÄ'*70}\n\n"
        
        for source, source_articles in sorted(
            groups['by_source'].items(),
            key=lambda x: len(x[1]),
            reverse=True
        ):
            report += f"  ‚Ä¢ {source:30} ‚Üí {len(source_articles):3} —Å—Ç–∞—Ç–µ–π\n"
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—Ç–æ–ø-—Ç–µ–º—ã)
        if groups['by_keyword']:
            report += f"\n\n{'‚îÄ'*70}\n"
            report += "–¢–û–ü –¢–ï–ú–´ (–ø–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤)\n"
            report += f"{'‚îÄ'*70}\n\n"
            
            for keyword, kw_articles in sorted(
                groups['by_keyword'].items(),
                key=lambda x: len(x[1]),
                reverse=True
            )[:10]:  # –¢–æ–ø-10 —Ç–µ–º
                report += f"  üìå {keyword} ‚Üí {len(kw_articles)} —Å—Ç–∞—Ç–µ–π\n"
        
        # –í—Å–µ —Å—Ç–∞—Ç—å–∏ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        report += f"\n\n{'='*70}\n"
        report += "–í–°–ï –°–¢–ê–¢–¨–ò (–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º)\n"
        report += f"{'='*70}\n"
        
        for source, source_articles in sorted(groups['by_source'].items()):
            report += f"\n{'‚îÄ'*70}\n"
            report += f"üì∞ {source.upper()} ({len(source_articles)} —Å—Ç–∞—Ç–µ–π)\n"
            report += f"{'‚îÄ'*70}\n\n"
            
            for i, article in enumerate(sorted(
                source_articles,
                key=lambda x: x['published'],
                reverse=True
            ), 1):
                pub_time = datetime.fromisoformat(article['published'])
                report += f"{i}. {article['title']}\n"
                report += f"   üîó {article['url']}\n"
                report += f"   üìÖ {pub_time.strftime('%d.%m.%Y %H:%M')}\n"
                
                if article.get('matched_keywords'):
                    report += f"   üè∑Ô∏è  –¢–µ–º—ã: {', '.join(article['matched_keywords'])}\n"
                
                if article['description']:
                    desc = article['description'][:200].replace('\n', ' ')
                    report += f"   üìù {desc}...\n"
                
                report += "\n"
        
        # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        report += f"\n{'='*70}\n"
        report += "–°–õ–ï–î–£–Æ–©–ò–ô –®–ê–ì: –ê–ù–ê–õ–ò–ó\n"
        report += f"{'='*70}\n\n"
        report += "–î–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏:\n\n"
        report += "1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª raw_articles_latest.json –≤ —á–∞—Ç —Å Claude\n"
        report += "2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª criteria.yaml –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n"
        report += "3. –ü–æ–ø—Ä–æ—Å–∏—Ç–µ Claude:\n"
        report += '   "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É—è –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏–∑ criteria.yaml.\n'
        report += '    –°–≥—Ä—É–ø–ø–∏—Ä—É–π –ø–æ —Ç–µ–º–∞–º, –æ—Ü–µ–Ω–∏ –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –∫–∞–∂–¥–æ–π —Ç–µ–º—ã, —Å–æ–∑–¥–∞–π –æ—Ç—á–µ—Ç."\n\n'
        report += "4. Claude —Å–æ–∑–¥–∞—Å—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç —Å –æ—Ü–µ–Ω–∫–∞–º–∏\n"
        report += "5. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∫ analyzed_digest_[–¥–∞—Ç–∞].txt\n\n"
        
        return report
    
    def save_reports(self, articles: List[dict], groups: dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –æ—Ç—á–µ—Ç–æ–≤"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ—Å–∫–æ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è –¥–ª—è –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤
        moscow_tz = pytz.timezone('Europe/Moscow')
        timestamp = datetime.now(moscow_tz).strftime('%Y-%m-%d_%H-%M')
        
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ JSON
        json_path = f"reports/raw_articles_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': timestamp,
                'total_articles': len(articles),
                'articles': articles,
                'groups': {
                    'by_source': {k: len(v) for k, v in groups['by_source'].items()},
                    'by_category': {k: len(v) for k, v in groups['by_category'].items()},
                    'by_keyword': {k: len(v) for k, v in groups['by_keyword'].items()}
                }
            }, f, ensure_ascii=False, indent=2)
        
        # 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
        text_report = self.generate_text_report(articles, groups)
        txt_path = f"reports/raw_digest_{timestamp}.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(text_report)
        
        # 3. –û–±–Ω–æ–≤–ª—è–µ–º latest.txt
        with open("reports/latest.txt", 'w', encoding='utf-8') as f:
            f.write(text_report)
        
        # 4. –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π JSON –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
        summary_path = f"reports/summary_{timestamp}.json"
        
        # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è summary
        if groups['by_keyword']:
            top_items = sorted(
                [(k, len(v)) for k, v in groups['by_keyword'].items()],
                key=lambda x: x[1],
                reverse=True
            )[:20]
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            top_items = sorted(
                [(k, len(v)) for k, v in groups['by_source'].items()],
                key=lambda x: x[1],
                reverse=True
            )[:10]
        
        summary_data = {
            'timestamp': timestamp,
            'total_articles': len(articles),
            'sources': {k: len(v) for k, v in groups['by_source'].items()},
            'top_keywords': top_items,
            'recent_headlines': [
                {
                    'title': a['title'],
                    'source': a['source'],
                    'url': a['url'],
                    'published': a['published']
                }
                for a in sorted(
                    articles,
                    key=lambda x: x['published'],
                    reverse=True
                )[:10]
            ]
        }
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        # 5. –í–ê–ñ–ù–û: –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏–∏ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏ –¥–ª—è –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
        shutil.copy(summary_path, "reports/summary_latest.json")
        shutil.copy(json_path, "reports/raw_articles_latest.json")
        
        print(f"{'='*70}")
        print(f"‚úì –û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"  ‚Ä¢ –î–∞–Ω–Ω—ã–µ (JSON):    {json_path}")
        print(f"  ‚Ä¢ –û—Ç—á–µ—Ç (—Ç–µ–∫—Å—Ç):    {txt_path}")
        print(f"  ‚Ä¢ –°–≤–æ–¥–∫–∞ (JSON):    {summary_path}")
        print(f"  ‚Ä¢ –ü–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç—á–µ—Ç:  reports/latest.txt")
        print(f"  ‚Ä¢ –î–ª—è –≤–µ–±:          reports/summary_latest.json")
        print(f"  ‚Ä¢ –î–ª—è –≤–µ–±:          reports/raw_articles_latest.json")
        print(f"{'='*70}\n")
        
        return json_path, txt_path
    
    def run(self):
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞"""
        moscow_tz = pytz.timezone('Europe/Moscow')
        print(f"\n{'='*70}")
        print(f"–ó–ê–ü–£–°–ö –ù–û–í–û–°–¢–ù–û–ì–û –ê–ì–†–ï–ì–ê–¢–û–†–ê")
        print(f"{'='*70}")
        print(f"–í—Ä–µ–º—è (–ú–°–ö): {datetime.now(moscow_tz).strftime('%d.%m.%Y %H:%M:%S')}\n")
    
        # –°–æ–±–∏—Ä–∞–µ–º –∏–∑ RSS
        hours_back = self.feeds_config.get('filters', {}).get('hours_back', 24)
        rss_articles = self.fetch_all_news(hours_back)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑ Google News
        google_articles = self.fetch_google_news(hours_back)
        all_articles = rss_articles + google_articles
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º
        filtered_articles = self.filter_by_keywords(all_articles)
        
        if not filtered_articles:
            print("‚úó –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\n")
            return
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º
        groups = self.group_articles(filtered_articles)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        json_path, txt_path = self.save_reports(filtered_articles, groups)
        
        print("\n–ì–æ—Ç–æ–≤–æ! –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print("1. –ü—Ä–æ—Å–º–æ—Ç—Ä–∏—Ç–µ reports/latest.txt –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏—è")
        print(f"2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ reports/raw_articles_latest.json –≤ Claude –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        print("3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ criteria.yaml –∫–∞–∫ reference –¥–ª—è –æ—Ü–µ–Ω–∫–∏\n")


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    aggregator = NewsAggregator(
        feeds_config='feeds.yaml',
        criteria_config='criteria.yaml'
    )
    aggregator.run()


if __name__ == "__main__":
    main()
